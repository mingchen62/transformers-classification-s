{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'num_train_epochs': 1, 'save_steps':1000, 'train_batch_size':32, 'eval_batch_size':8}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#git_config = {'repo': 'https://github.com/awslabs/amazon-sagemaker-examples.git', 'branch': 'training-scripts'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point='train.py',\n",
    "                    role=role,\n",
    "                    framework_version='1.1.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p2.8xlarge',\n",
    "                    source_dir='email-compliance-bert',\n",
    "                    #git_config=git_config,\n",
    "                    hyperparameters=hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-15 12:42:23 Starting - Starting the training job...\n",
      "2019-09-15 12:42:24 Starting - Launching requested ML instances......\n",
      "2019-09-15 12:43:53 Starting - Preparing the instances for training.........\n",
      "2019-09-15 12:45:20 Downloading - Downloading input data\n",
      "2019-09-15 12:45:20 Training - Downloading the training image...\n",
      "2019-09-15 12:45:49 Training - Training image download completed. Training in progress..\u001b[31mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[31mbash: no job control in this shell\u001b[0m\n",
      "\u001b[31m2019-09-15 12:45:50,949 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[31m2019-09-15 12:45:51,029 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[31m2019-09-15 12:45:54,043 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[31m2019-09-15 12:45:54,296 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[31mGenerating setup.py\u001b[0m\n",
      "\u001b[31m2019-09-15 12:45:54,296 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[31m2019-09-15 12:45:54,296 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[31m2019-09-15 12:45:54,296 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[31mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[31mCollecting sklearn (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\u001b[0m\n",
      "\u001b[31mCollecting pytorch-transformers (from -r requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\u001b[0m\n",
      "\u001b[31mCollecting scikit-learn (from sklearn->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[31m  Downloading https://files.pythonhosted.org/packages/a0/c5/d2238762d780dde84a20b8c761f563fe882b88c5a5fb03c056547c442a19/scikit_learn-0.21.3-cp36-cp36m-manylinux1_x86_64.whl (6.7MB)\u001b[0m\n",
      "\u001b[31mCollecting sentencepiece (from pytorch-transformers->-r requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\u001b[0m\n",
      "\u001b[31mCollecting sacremoses (from pytorch-transformers->-r requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/df/24/0b86f494d3a5c7531f6d0c77d39fd8f9d42e651244505d3d737e31db9a4d/sacremoses-0.0.33.tar.gz (802kB)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers->-r requirements.txt (line 2)) (1.9.209)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers->-r requirements.txt (line 2)) (1.16.4)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers->-r requirements.txt (line 2)) (4.33.0)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers->-r requirements.txt (line 2)) (1.1.0)\u001b[0m\n",
      "\u001b[31mCollecting regex (from pytorch-transformers->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[31m  Downloading https://files.pythonhosted.org/packages/6f/a6/99eeb5904ab763db87af4bd71d9b1dfdd9792681240657a4c0a599c10a81/regex-2019.08.19.tar.gz (654kB)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers->-r requirements.txt (line 2)) (2.22.0)\u001b[0m\n",
      "\u001b[31mCollecting joblib>=0.11 (from scikit-learn->sklearn->-r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/cd/c1/50a758e8247561e58cb87305b1e90b171b8c767b15b12a1734001f41d356/joblib-0.13.2-py2.py3-none-any.whl (278kB)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->-r requirements.txt (line 1)) (1.3.1)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers->-r requirements.txt (line 2)) (1.12.0)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers->-r requirements.txt (line 2)) (7.0)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers->-r requirements.txt (line 2)) (0.9.4)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: botocore<1.13.0,>=1.12.209 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers->-r requirements.txt (line 2)) (1.12.209)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers->-r requirements.txt (line 2)) (0.2.1)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers->-r requirements.txt (line 2)) (2.8)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers->-r requirements.txt (line 2)) (3.0.4)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers->-r requirements.txt (line 2)) (2019.6.16)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers->-r requirements.txt (line 2)) (1.25.3)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.209->boto3->pytorch-transformers->-r requirements.txt (line 2)) (0.15.2)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.209->boto3->pytorch-transformers->-r requirements.txt (line 2)) (2.8.0)\u001b[0m\n",
      "\u001b[31mBuilding wheels for collected packages: sklearn, train, sacremoses, regex\n",
      "  Running setup.py bdist_wheel for sklearn: started\u001b[0m\n",
      "\u001b[31m  Running setup.py bdist_wheel for sklearn: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/76/03/bb/589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
      "  Running setup.py bdist_wheel for train: started\n",
      "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-z8o_yfek/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\n",
      "  Running setup.py bdist_wheel for sacremoses: started\u001b[0m\n",
      "\u001b[31m  Running setup.py bdist_wheel for sacremoses: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/70/87/56/e40575cca30d12fee8875d523b8878b7aba866a9f03b2fd983\n",
      "  Running setup.py bdist_wheel for regex: started\u001b[0m\n",
      "\u001b[31m  Running setup.py bdist_wheel for regex: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/90/04/07/b5010fb816721eb3d6dd64ed5cc8111ca23f97fdab8619b5be\u001b[0m\n",
      "\u001b[31mSuccessfully built sklearn train sacremoses regex\u001b[0m\n",
      "\u001b[31mInstalling collected packages: joblib, scikit-learn, sklearn, sentencepiece, sacremoses, regex, pytorch-transformers, train\u001b[0m\n",
      "\u001b[31mSuccessfully installed joblib-0.13.2 pytorch-transformers-1.2.0 regex-2019.8.19 sacremoses-0.0.33 scikit-learn-0.21.3 sentencepiece-0.1.83 sklearn-0.0 train-1.0.0\u001b[0m\n",
      "\u001b[31mYou are using pip version 18.1, however version 19.2.3 is available.\u001b[0m\n",
      "\u001b[31mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31m2019-09-15 12:46:09,158 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[31mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[31m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"train_batch_size\": 32,\n",
      "        \"num_train_epochs\": 1,\n",
      "        \"save_steps\": 1000,\n",
      "        \"eval_batch_size\": 8\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-pytorch-2019-09-15-12-42-22-750\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-517600888691/sagemaker-pytorch-2019-09-15-12-42-22-750/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 32,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[31m}\n",
      "\u001b[0m\n",
      "\u001b[31mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[31mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[31mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[31mSM_HPS={\"eval_batch_size\":8,\"num_train_epochs\":1,\"save_steps\":1000,\"train_batch_size\":32}\u001b[0m\n",
      "\u001b[31mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[31mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[31mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[31mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[31mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[31mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31mSM_NUM_CPUS=32\u001b[0m\n",
      "\u001b[31mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[31mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31mSM_MODULE_DIR=s3://sagemaker-us-east-1-517600888691/sagemaker-pytorch-2019-09-15-12-42-22-750/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[31mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"eval_batch_size\":8,\"num_train_epochs\":1,\"save_steps\":1000,\"train_batch_size\":32},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-pytorch-2019-09-15-12-42-22-750\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-517600888691/sagemaker-pytorch-2019-09-15-12-42-22-750/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[31mSM_USER_ARGS=[\"--eval_batch_size\",\"8\",\"--num_train_epochs\",\"1\",\"--save_steps\",\"1000\",\"--train_batch_size\",\"32\"]\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[31mSM_HP_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[31mSM_HP_NUM_TRAIN_EPOCHS=1\u001b[0m\n",
      "\u001b[31mSM_HP_SAVE_STEPS=1000\u001b[0m\n",
      "\u001b[31mSM_HP_EVAL_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[31mPYTHONPATH=/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[31mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m train --eval_batch_size 8 --num_train_epochs 1 --save_steps 1000 --train_batch_size 32\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31mNamespace(adam_epsilon=1e-08, current_host='algo-1', data_dir='/opt/ml/input/data/training', eval_batch_size=8, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, hosts=['algo-1'], learning_rate=4e-05, logging_steps=500, max_grad_norm=1.0, max_seq_length=512, model_dir='/opt/ml/model', model_name='bert-base-uncased', model_type='bert', num_gpus=8, num_train_epochs=1, output_dir='./outputs', output_mode='classification', reprocess_input_data=False, save_steps=1000, task_name='binary', train_batch_size=32, warmup_steps=0, weight_decay=0, workers=2)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mTraining: use 8 GPUs!\u001b[0m\n",
      "\u001b[31mlen(train_dataloader) 1329\u001b[0m\n",
      "\u001b[31m#0150.354053\u001b[0m\n",
      "\u001b[31malgo-1:97:169 [0] misc/ibvwrap.cu:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[31mNCCL version 2.4.2+cuda9.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "estimator.fit('s3://sagemaker-us-east-1-517600888691/compliance-data/batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls s3://sagemaker-us-east-1-517600888691/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import model into SageMaker\n",
    "The PyTorch model uses a npy serializer and deserializer by default. since we have a custom implementation of all the hosting functions and plan on using JSON instead, we need a predictor that can serialize and deserialize JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import RealTimePredictor, json_serializer, json_deserializer\n",
    "\n",
    "class JSONPredictor(RealTimePredictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(JSONPredictor, self).__init__(endpoint_name, sagemaker_session, json_serializer, json_deserializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since hosting functions implemented outside of train script we can't just use estimator object to deploy the model. Instead we need to create a PyTorchModel object using the latest training job to get the S3 location of the trained model data. Besides model data location in S3, we also need to configure PyTorchModel with the script and source directory (because our generate script requires model and data classes from source directory), an IAM role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "training_job_name = estimator.latest_training_job.name\n",
    "desc = sagemaker_session.sagemaker_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "trained_model_location = desc['ModelArtifacts']['S3ModelArtifacts']\n",
    "model = PyTorchModel(model_data=trained_model_location,\n",
    "                     role=role,\n",
    "                     framework_version='1.1.0',\n",
    "                     entry_point='inference.py',\n",
    "                     source_dir='email-compliance-bert',\n",
    "                     #git_config=git_config,\n",
    "                     predictor_cls=JSONPredictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create endpoint\n",
    "Now the model is ready to be deployed at a SageMaker endpoint and we are going to use the sagemaker.pytorch.model.PyTorchModel.deploy method to do this. We can use a CPU-based instance for inference (in this case an ml.m4.xlarge), even though we trained on GPU instances, because at the end of training we moved model to cpu before returning it. This way we can load trained model on any device and then move to GPU if CUDA is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.p2.8xlarge')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-pytorch-2019-09-15-14-22-24-975'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello_song=\"\"\"\n",
    "Hello, it's me.\n",
    "I was wondering if after all these years you'd like to meet.\n",
    "To go over everything.\n",
    "They say that time's supposed to heal you.\n",
    "But I ain't done much healing.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('response=', u'0', 0.11596512794494629)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_t=time.time()\n",
    "input_json = {\n",
    "    'txt': hello_song\n",
    "}\n",
    "d=json.dumps(input_json)\n",
    "response = predictor.predict(input_json)\n",
    "print(\"response=\", response, time.time()-start_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('response=', {u'InvokedProductionVariant': 'AllTraffic', u'Body': <botocore.response.StreamingBody object at 0x7f72c7872d10>, u'ContentType': 'application/json', 'ResponseMetadata': {'RetryAttempts': 0, 'HTTPStatusCode': 200, 'RequestId': '5e826411-0d9a-4d1d-bef5-104797628d05', 'HTTPHeaders': {'x-amzn-requestid': '5e826411-0d9a-4d1d-bef5-104797628d05', 'x-amzn-invoked-production-variant': 'AllTraffic', 'content-length': '3', 'date': 'Sun, 15 Sep 2019 14:38:16 GMT', 'content-type': 'application/json'}}}, 0.19006991386413574)\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "runtime = boto3.Session().client(service_name='runtime.sagemaker',region_name='us-east-1')\n",
    "#endpoint_name = 'sagemaker-pytorch-2019-09-15-13-33-39-536'\n",
    "endpoint_name = 'sagemaker-pytorch-2019-09-15-14-22-24-975'\n",
    "start_t=time.time()\n",
    "response = runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    " ContentType='application/json',\n",
    " Body=d)\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "print(\"response=\", response, time.time()-start_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanup\n",
    "To delete the prediction endpoint to release the instance(s) associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.delete_endpoint(predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p27",
   "language": "python",
   "name": "conda_pytorch_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
